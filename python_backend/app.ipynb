{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests)\n",
      "  Downloading charset_normalizer-3.3.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests)\n",
      "  Downloading idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests)\n",
      "  Downloading urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests)\n",
      "  Downloading certifi-2024.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Downloading certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading charset_normalizer-3.3.2-cp311-cp311-macosx_11_0_arm64.whl (118 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.0/119.0 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading idna-3.6-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: urllib3, idna, charset-normalizer, certifi, requests\n",
      "Successfully installed certifi-2024.2.2 charset-normalizer-3.3.2 idna-3.6 requests-2.31.0 urllib3-2.2.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Collecting beautifulsoup4 (from bs4)\n",
      "  Downloading beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->bs4)\n",
      "  Using cached soupsieve-2.5-py3-none-any.whl.metadata (4.7 kB)\n",
      "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Downloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.9/147.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached soupsieve-2.5-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4, bs4\n",
      "Successfully installed beautifulsoup4-4.12.3 bs4-0.0.2 soupsieve-2.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.2.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (19 kB)\n",
      "Collecting numpy<2,>=1.23.2 (from pandas)\n",
      "  Downloading numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.8/114.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /Users/christopherdedow/clones/quill/.venv/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/christopherdedow/clones/quill/.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pandas-2.2.1-cp311-cp311-macosx_11_0_arm64.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl (14.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m505.5/505.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.4/345.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pytz, tzdata, numpy, pandas\n",
      "Successfully installed numpy-1.26.4 pandas-2.2.1 pytz-2024.1 tzdata-2024.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting psycopg2\n",
      "  Downloading psycopg2-2.9.9.tar.gz (384 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m384.9/384.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Building wheels for collected packages: psycopg2\n",
      "  Building wheel for psycopg2 (pyproject.toml): started\n",
      "  Building wheel for psycopg2 (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for psycopg2: filename=psycopg2-2.9.9-cp311-cp311-macosx_14_0_arm64.whl size=144161 sha256=8d019480bdbc780e616040a7ccc866cf077f370a5632b9cb5505de62596e5979\n",
      "  Stored in directory: /Users/christopherdedow/Library/Caches/pip/wheels/ab/34/b9/78ebef1b3220b4840ee482461e738566c3c9165d2b5c914f51\n",
      "Successfully built psycopg2\n",
      "Installing collected packages: psycopg2\n",
      "Successfully installed psycopg2-2.9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sqlalchemy\n",
      "  Downloading SQLAlchemy-2.0.28-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.6 kB)\n",
      "Collecting typing-extensions>=4.6.0 (from sqlalchemy)\n",
      "  Downloading typing_extensions-4.10.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Downloading SQLAlchemy-2.0.28-cp311-cp311-macosx_11_0_arm64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.10.0-py3-none-any.whl (33 kB)\n",
      "Installing collected packages: typing-extensions, sqlalchemy\n",
      "Successfully installed sqlalchemy-2.0.28 typing-extensions-4.10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dotenv\n",
      "  Downloading dotenv-0.0.5.tar.gz (2.4 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpip subprocess to install backend dependencies\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[34 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Collecting distribute\n",
      "  \u001b[31m   \u001b[0m   Downloading distribute-0.7.3.zip (145 kB)\n",
      "  \u001b[31m   \u001b[0m \u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/145.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.4/145.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \u001b[?25h  Installing build dependencies: started\n",
      "  \u001b[31m   \u001b[0m   Installing build dependencies: finished with status 'done'\n",
      "  \u001b[31m   \u001b[0m   Getting requirements to build wheel: started\n",
      "  \u001b[31m   \u001b[0m   Getting requirements to build wheel: finished with status 'done'\n",
      "  \u001b[31m   \u001b[0m   Preparing metadata (pyproject.toml): started\n",
      "  \u001b[31m   \u001b[0m   Preparing metadata (pyproject.toml): finished with status 'error'\n",
      "  \u001b[31m   \u001b[0m   \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m   \u001b[31m×\u001b[0m \u001b[32mPreparing metadata \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m╰─>\u001b[0m \u001b[31m[6 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m usage: setup.py [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m    or: setup.py --help [cmd1 cmd2 ...]\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m    or: setup.py --help-commands\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m    or: setup.py cmd --help\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m error: invalid command 'dist_info'\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m   \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  \u001b[31m   \u001b[0m \u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "  \u001b[31m   \u001b[0m \u001b[31m╰─>\u001b[0m See above for output.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "  \u001b[31m   \u001b[0m \u001b[1;36mhint\u001b[0m: See above for details.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m \u001b[32mpip subprocess to install backend dependencies\u001b[0m did not run successfully.\n",
      "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['/Users/christopherdedow/clones/quill/.venv/bin/python', '-m', 'pip', 'install', 'dotenv']' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 12\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;43m__import__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dotenv'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28m__import__\u001b[39m(package)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m---> 14\u001b[0m     \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m-m\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minstall\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError installing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpackage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.7_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/subprocess.py:413\u001b[0m, in \u001b[0;36mcheck_call\u001b[0;34m(*popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cmd \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    412\u001b[0m         cmd \u001b[38;5;241m=\u001b[39m popenargs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 413\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(retcode, cmd)\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '['/Users/christopherdedow/clones/quill/.venv/bin/python', '-m', 'pip', 'install', 'dotenv']' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "# run code to check if packages are installed from requirements.txt and if not install them\n",
    "\n",
    "requirements = open('requirements.txt', 'r')\n",
    "requirements = requirements.read()\n",
    "requirements = requirements.split('\\n')\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "for package in requirements:\n",
    "    try:\n",
    "        __import__(package)\n",
    "    except ImportError:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "    except:\n",
    "        print(f'Error installing {package}')\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christopherdedow/clones/quill/.venv/lib/python3.11/site-packages/urllib3/connectionpool.py:1103: InsecureRequestWarning: Unverified HTTPS request is being made to host '3000mostcommonwords.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# create dictionary of all the 3,000 most common words for each language found at the following url\n",
    "# https://3000mostcommonwords.com/ \n",
    "# the dictionary should store the key as the language and the corresponding dataframe as the values found from the url\n",
    "\n",
    "# create a function that takes in a string and returns the language of the string\n",
    "# the function should use the dictionary created above to determine the language of the string\n",
    "# the function should return the language of the string\n",
    "\n",
    "# below is an example url to use\n",
    "# https://3000mostcommonwords.com/list-of-3000-most-common-portuguese-brazil-words-in-english/\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://3000mostcommonwords.com/list-of-3000-most-common-portuguese-brazil-words-in-english/\"\n",
    "\n",
    "# response = requests.get(url)\n",
    "response = requests.get('https://3000mostcommonwords.com/list-of-3000-most-common-portuguese-brazil-words-in-english/', verify=False)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Depending on the structure of the webpage, you'd find the table and extract the words.\n",
    "# This is a general approach and might need adjustments based on the actual webpage structure.\n",
    "table = soup.find('table')  # find the table in the webpage\n",
    "# save table as a dataframe\n",
    "# table_df = pd.read_html(str(table))[0]\n",
    "# table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty dataframe with the column names from the first row of the table\n",
    "\n",
    "# Get the header row\n",
    "header_row = table.find_all('tr')[0]\n",
    "columns = header_row.find_all('th')\n",
    "column_names = [c.text.strip() for c in columns]\n",
    "\n",
    "# Initialize the DataFrame\n",
    "words_df = pd.DataFrame(columns=column_names)\n",
    "# words_df\n",
    "# Populate the DataFrame\n",
    "for idx, row in enumerate(table.find_all('tr')[1:]):  # skip the header row\n",
    "    columns = row.find_all('td')\n",
    "    data = [c.text.strip() for c in columns]\n",
    "    words_df.loc[idx] = data\n",
    "\n",
    "# Now, you can reference words_df outside of the loop\n",
    "# words_df.head()\n",
    "\n",
    "\n",
    "\n",
    "# def get_language(string):\n",
    "    # \n",
    "    # pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2999 entries, 0 to 2998\n",
      "Data columns (total 5 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   word_id          2999 non-null   int64 \n",
      " 1   word_english     2999 non-null   object\n",
      " 2   part_of_speech   2999 non-null   object\n",
      " 3   level            2999 non-null   object\n",
      " 4   word_portuguese  2999 non-null   object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 140.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# change column Sr to integer\n",
    "words_df['Sr'] = words_df['Sr'].astype(int)\n",
    "words_df.rename(columns={'Sr': 'word_id', 'Words': 'word_english', 'Portuguese  Mearning': 'word_portuguese', 'P.O.S': 'part_of_speech'}, inplace=True)\n",
    "# make all columns lowercase\n",
    "words_df.columns = words_df.columns.str.lower()\n",
    "words_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_id</th>\n",
       "      <th>word_english</th>\n",
       "      <th>part_of_speech</th>\n",
       "      <th>level</th>\n",
       "      <th>word_portuguese</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>a, an</td>\n",
       "      <td>indefinite, article</td>\n",
       "      <td>A1</td>\n",
       "      <td>um, um</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>abandon</td>\n",
       "      <td>Verb</td>\n",
       "      <td>B2</td>\n",
       "      <td>abandono</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>ability</td>\n",
       "      <td>Noun</td>\n",
       "      <td>A2</td>\n",
       "      <td>habilidade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>able</td>\n",
       "      <td>Adjective</td>\n",
       "      <td>A2</td>\n",
       "      <td>capaz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>about</td>\n",
       "      <td>prep. Adverb</td>\n",
       "      <td>A1</td>\n",
       "      <td>sobre</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_id word_english       part_of_speech level word_portuguese\n",
       "0        1        a, an  indefinite, article    A1          um, um\n",
       "1        2      abandon                 Verb    B2        abandono\n",
       "2        3      ability                 Noun    A2      habilidade\n",
       "3        4         able            Adjective    A2           capaz\n",
       "4        5        about         prep. Adverb    A1           sobre"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in words_df['Sr'] find the number that is missing in the list between 1 and 3000\n",
    "# the number that is missing is the number of the word that is missing from the list\n",
    "# for i in range(0, 3000):\n",
    "#     if i not in words_df['word_id'].values:\n",
    "#         print(i)\n",
    "words_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in column values for the word 'Noun' that had missing values\n",
    "mask = words_df['level'] == ''\n",
    "\n",
    "# Update each column for those rows\n",
    "words_df.loc[mask, 'part_of_speech'] = 'Noun'\n",
    "words_df.loc[mask, 'level'] = 'B1'\n",
    "# words_df[words_df['words'] == 'Noun']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "level\n",
       "A1    901\n",
       "A2    799\n",
       "B1    698\n",
       "B2    601\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Remove all spaces (including in the middle) from the 'Level' column and split the string by commas to return the first value\n",
    "words_df['level'] = words_df['level'].str.replace(' ', '').str.split(',').str[0]\n",
    "\n",
    "# return only one value in 'words' and 'portuguese_meaning' columns for ease of tracking\n",
    "words_df['word_english'] = words_df['word_english'].str.split(',').str[0]\n",
    "words_df['word_portuguese'] = words_df['word_portuguese'].str.split(',').str[0]\n",
    "# change value where words_df['Level_lowest'] == 'Noun' to 'A1'\n",
    "words_df.loc[words_df['level'] == 'Noun', 'level'] = 'A1'\n",
    "words_df['level'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_id</th>\n",
       "      <th>word_english</th>\n",
       "      <th>part_of_speech</th>\n",
       "      <th>level</th>\n",
       "      <th>word_portuguese</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>indefinite, article</td>\n",
       "      <td>A1</td>\n",
       "      <td>um</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>abandon</td>\n",
       "      <td>Verb</td>\n",
       "      <td>B2</td>\n",
       "      <td>abandono</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>ability</td>\n",
       "      <td>Noun</td>\n",
       "      <td>A2</td>\n",
       "      <td>habilidade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>able</td>\n",
       "      <td>Adjective</td>\n",
       "      <td>A2</td>\n",
       "      <td>capaz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>about</td>\n",
       "      <td>prep. Adverb</td>\n",
       "      <td>A1</td>\n",
       "      <td>sobre</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_id word_english       part_of_speech level word_portuguese\n",
       "0        1            a  indefinite, article    A1              um\n",
       "1        2      abandon                 Verb    B2        abandono\n",
       "2        3      ability                 Noun    A2      habilidade\n",
       "3        4         able            Adjective    A2           capaz\n",
       "4        5        about         prep. Adverb    A1           sobre"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make all words in the 'words' and 'portuguese meaning' column lowercase\n",
    "words_df['word_english'] = words_df['word_english'].str.lower()\n",
    "words_df['word_portuguese'] = words_df['word_portuguese'].str.lower()\n",
    "words_df['part_of_speech'] = words_df['part_of_speech'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-16 15:39:27,325 INFO sqlalchemy.engine.Engine select pg_catalog.version()\n",
      "2024-03-16 15:39:27,326 INFO sqlalchemy.engine.Engine [raw sql] {}\n",
      "2024-03-16 15:39:27,370 INFO sqlalchemy.engine.Engine select current_schema()\n",
      "2024-03-16 15:39:27,371 INFO sqlalchemy.engine.Engine [raw sql] {}\n",
      "2024-03-16 15:39:27,398 INFO sqlalchemy.engine.Engine show standard_conforming_strings\n",
      "2024-03-16 15:39:27,398 INFO sqlalchemy.engine.Engine [raw sql] {}\n"
     ]
    }
   ],
   "source": [
    "# connect to neon.tech database and create a table for the words_df dataframe in the database\n",
    "import psycopg2\n",
    "# from sqlalchemy import create_engine\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# connection details for main\n",
    "from sqlalchemy import URL, create_engine\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# Access environment variables\n",
    "connection_string = os.getenv('DATABASE_URL')\n",
    "\n",
    "# connect to the database\n",
    "engine = create_engine(connection_string, echo=True)\n",
    "connection = engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-16 15:40:20,110 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2024-03-16 15:40:20,122 INFO sqlalchemy.engine.Engine \n",
      "select *\n",
      "from \"Message\"\n",
      "limit 5;\n",
      "\n",
      "2024-03-16 15:40:20,123 INFO sqlalchemy.engine.Engine [generated in 0.00110s] {}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method Connection.rollback of <sqlalchemy.engine.base.Connection object at 0x2816b8dd0>>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sqlalchemy import text\n",
    "\n",
    "query_message_table = text(\"\"\"\n",
    "select *\n",
    "from \"Message\"\n",
    "limit 5;\n",
    "\"\"\")\n",
    "\n",
    "# execute the query\n",
    "connection.begin()\n",
    "result = connection.execute(query_message_table)\n",
    "# fetch the result\n",
    "result.fetchall()\n",
    "connection.rollback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3l/0kvmql1518x0p2_k9tq2f_lw0000gn/T/ipykernel_98205/96557568.py:5: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)\n",
      "  Base = declarative_base()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-16 15:27:56,707 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2024-03-16 15:27:56,708 INFO sqlalchemy.engine.Engine SELECT pg_catalog.pg_class.relname \n",
      "FROM pg_catalog.pg_class JOIN pg_catalog.pg_namespace ON pg_catalog.pg_namespace.oid = pg_catalog.pg_class.relnamespace \n",
      "WHERE pg_catalog.pg_class.relname = %(table_name)s AND pg_catalog.pg_class.relkind = ANY (ARRAY[%(param_1)s, %(param_2)s, %(param_3)s, %(param_4)s, %(param_5)s]) AND pg_catalog.pg_table_is_visible(pg_catalog.pg_class.oid) AND pg_catalog.pg_namespace.nspname != %(nspname_1)s\n",
      "2024-03-16 15:27:56,708 INFO sqlalchemy.engine.Engine [cached since 723.7s ago] {'table_name': 'word', 'param_1': 'r', 'param_2': 'p', 'param_3': 'f', 'param_4': 'v', 'param_5': 'm', 'nspname_1': 'pg_catalog'}\n",
      "2024-03-16 15:27:56,748 INFO sqlalchemy.engine.Engine \n",
      "CREATE TABLE word (\n",
      "\tword_id SERIAL NOT NULL, \n",
      "\tword_english VARCHAR, \n",
      "\tpart_of_speech VARCHAR, \n",
      "\tlevel VARCHAR, \n",
      "\tword_portuguese VARCHAR, \n",
      "\tPRIMARY KEY (word_id)\n",
      ")\n",
      "\n",
      "\n",
      "2024-03-16 15:27:56,748 INFO sqlalchemy.engine.Engine [no key 0.00057s] {}\n",
      "2024-03-16 15:27:56,779 INFO sqlalchemy.engine.Engine COMMIT\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine, Column, Integer, String\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "class Word(Base):\n",
    "    __tablename__ = 'word'\n",
    "    word_id = Column(Integer, primary_key=True, autoincrement=True)  # Corrected for PostgreSQL\n",
    "    word_english = Column(String)\n",
    "    part_of_speech = Column(String)\n",
    "    level = Column(String)\n",
    "    word_portuguese = Column(String)\n",
    "\n",
    "\n",
    "Base.metadata.create_all(engine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-16 15:29:08,859 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2024-03-16 15:29:08,886 INFO sqlalchemy.engine.Engine INSERT INTO word (word_english, part_of_speech, level, word_portuguese) SELECT p0::VARCHAR, p1::VARCHAR, p2::VARCHAR, p3::VARCHAR FROM (VALUES (%(word_english__0)s, %(part_of_speech__0)s, %(level__0)s, %(word_portuguese__0)s, 0), (%(word_english__1)s ... 96354 characters truncated ... p0, p1, p2, p3, sen_counter) ORDER BY sen_counter RETURNING word.word_id, word.word_id AS word_id__1\n",
      "2024-03-16 15:29:08,888 INFO sqlalchemy.engine.Engine [generated in 0.00429s (insertmanyvalues) 1/3 (ordered)] {'word_english__0': 'a', 'part_of_speech__0': 'indefinite, article', 'level__0': 'A1', 'word_portuguese__0': 'um', 'word_english__1': 'abandon', 'part_of_speech__1': 'verb', 'level__1': 'B2', 'word_portuguese__1': 'abandono', 'word_english__2': 'ability', 'part_of_speech__2': 'noun', 'level__2': 'A2', 'word_portuguese__2': 'habilidade', 'word_english__3': 'able', 'part_of_speech__3': 'adjective', 'level__3': 'A2', 'word_portuguese__3': 'capaz', 'word_english__4': 'about', 'part_of_speech__4': 'prep. adverb', 'level__4': 'A1', 'word_portuguese__4': 'sobre', 'word_english__5': 'above', 'part_of_speech__5': 'prep. adverb', 'level__5': 'A1', 'word_portuguese__5': 'acima', 'word_english__6': 'abroad', 'part_of_speech__6': 'adverb', 'level__6': 'A2', 'word_portuguese__6': 'no exterior', 'word_english__7': 'absolute', 'part_of_speech__7': 'adjective', 'level__7': 'B2', 'word_portuguese__7': 'absoluto', 'word_english__8': 'absolutely', 'part_of_speech__8': 'adverb', 'level__8': 'B1', 'word_portuguese__8': 'absolutamente', 'word_english__9': 'academic', 'part_of_speech__9': 'adj, noun', 'level__9': 'B1', 'word_portuguese__9': 'acadêmico', 'word_english__10': 'accept', 'part_of_speech__10': 'verb', 'level__10': 'A2', 'word_portuguese__10': 'aceitar', 'word_english__11': 'acceptable', 'part_of_speech__11': 'adjective', 'level__11': 'B2', 'word_portuguese__11': 'aceitável', 'word_english__12': 'access', 'part_of_speech__12': 'noun verb' ... 3900 parameters truncated ... 'level__987': 'A2', 'word_portuguese__987': 'justo', 'word_english__988': 'fairly', 'part_of_speech__988': 'adverb', 'level__988': 'B1', 'word_portuguese__988': 'bastante', 'word_english__989': 'faith', 'part_of_speech__989': 'noun', 'level__989': 'B2', 'word_portuguese__989': 'fé', 'word_english__990': 'fall', 'part_of_speech__990': 'verb noun', 'level__990': 'A1', 'word_portuguese__990': 'cair', 'word_english__991': 'false', 'part_of_speech__991': 'adjective', 'level__991': 'A1', 'word_portuguese__991': 'falso', 'word_english__992': 'familiar', 'part_of_speech__992': 'adjective', 'level__992': 'B1', 'word_portuguese__992': 'familiar', 'word_english__993': 'family', 'part_of_speech__993': 'noun adjective', 'level__993': 'A1', 'word_portuguese__993': 'família', 'word_english__994': 'famous', 'part_of_speech__994': 'adjective', 'level__994': 'A1', 'word_portuguese__994': 'famoso', 'word_english__995': 'fan', 'part_of_speech__995': 'noun', 'level__995': 'A2', 'word_portuguese__995': 'ventilador', 'word_english__996': 'fancy', 'part_of_speech__996': 'verb adjective', 'level__996': 'B1', 'word_portuguese__996': 'chique', 'word_english__997': 'fantastic', 'part_of_speech__997': 'adjective', 'level__997': 'A1', 'word_portuguese__997': 'fantástico', 'word_english__998': 'far', 'part_of_speech__998': 'adverb , adjective', 'level__998': 'A1', 'word_portuguese__998': 'longe', 'word_english__999': 'farm', 'part_of_speech__999': 'noun verb', 'level__999': 'A1', 'word_portuguese__999': 'fazenda'}\n",
      "2024-03-16 15:29:09,024 INFO sqlalchemy.engine.Engine INSERT INTO word (word_english, part_of_speech, level, word_portuguese) SELECT p0::VARCHAR, p1::VARCHAR, p2::VARCHAR, p3::VARCHAR FROM (VALUES (%(word_english__0)s, %(part_of_speech__0)s, %(level__0)s, %(word_portuguese__0)s, 0), (%(word_english__1)s ... 96354 characters truncated ... p0, p1, p2, p3, sen_counter) ORDER BY sen_counter RETURNING word.word_id, word.word_id AS word_id__1\n",
      "2024-03-16 15:29:09,025 INFO sqlalchemy.engine.Engine [insertmanyvalues 2/3 (ordered)] {'word_english__0': 'farmer', 'part_of_speech__0': 'noun', 'level__0': 'A1', 'word_portuguese__0': 'agricultor', 'word_english__1': 'farming', 'part_of_speech__1': 'noun', 'level__1': 'A2', 'word_portuguese__1': 'agricultura', 'word_english__2': 'fascinating', 'part_of_speech__2': 'adjective', 'level__2': 'B1', 'word_portuguese__2': 'fascinante', 'word_english__3': 'fashion', 'part_of_speech__3': 'noun', 'level__3': 'A2', 'word_portuguese__3': 'moda', 'word_english__4': 'fashionable', 'part_of_speech__4': 'adjective', 'level__4': 'B1', 'word_portuguese__4': 'elegante', 'word_english__5': 'fast', 'part_of_speech__5': 'adjective, adverb', 'level__5': 'A1', 'word_portuguese__5': 'velozes', 'word_english__6': 'fasten', 'part_of_speech__6': 'verb', 'level__6': 'B1', 'word_portuguese__6': 'prender', 'word_english__7': 'fat', 'part_of_speech__7': 'adjective, noun', 'level__7': 'A1', 'word_portuguese__7': 'gordura', 'word_english__8': 'father', 'part_of_speech__8': 'noun', 'level__8': 'A1', 'word_portuguese__8': 'pai', 'word_english__9': 'fault', 'part_of_speech__9': 'noun', 'level__9': 'B2', 'word_portuguese__9': 'culpa', 'word_english__10': 'favour', 'part_of_speech__10': 'noun verb', 'level__10': 'B1', 'word_portuguese__10': 'favor', 'word_english__11': 'favourite', 'part_of_speech__11': 'adjective, noun', 'level__11': 'A1', 'word_portuguese__11': 'favorito', 'word_english__12': 'fear', 'part_of_speech__12': 'noun verb' ... 3900 parameters truncated ... 'level__987': 'B1', 'word_portuguese__987': 'retrato', 'word_english__988': 'pose', 'part_of_speech__988': 'verb', 'level__988': 'B2', 'word_portuguese__988': 'pose', 'word_english__989': 'position', 'part_of_speech__989': 'noun, verb', 'level__989': 'A2', 'word_portuguese__989': 'posição', 'word_english__990': 'positive', 'part_of_speech__990': 'adjective , noun', 'level__990': 'A1', 'word_portuguese__990': 'positivo', 'word_english__991': 'possess', 'part_of_speech__991': 'verb', 'level__991': 'B2', 'word_portuguese__991': 'possuir', 'word_english__992': 'possession', 'part_of_speech__992': 'noun', 'level__992': 'A2', 'word_portuguese__992': 'posse', 'word_english__993': 'possibility', 'part_of_speech__993': 'noun', 'level__993': 'A2', 'word_portuguese__993': 'possibilidade', 'word_english__994': 'possible', 'part_of_speech__994': 'adjective', 'level__994': 'A1', 'word_portuguese__994': 'possível', 'word_english__995': 'possibly', 'part_of_speech__995': 'adverb', 'level__995': 'B1', 'word_portuguese__995': 'possivelmente', 'word_english__996': 'post', 'part_of_speech__996': 'noun verb', 'level__996': 'A1', 'word_portuguese__996': 'postar', 'word_english__997': 'poster', 'part_of_speech__997': 'noun', 'level__997': 'A2', 'word_portuguese__997': 'poster', 'word_english__998': 'pot', 'part_of_speech__998': 'noun', 'level__998': 'B1', 'word_portuguese__998': 'panela', 'word_english__999': 'potato', 'part_of_speech__999': 'noun', 'level__999': 'A1', 'word_portuguese__999': 'batata'}\n",
      "2024-03-16 15:29:09,071 INFO sqlalchemy.engine.Engine INSERT INTO word (word_english, part_of_speech, level, word_portuguese) SELECT p0::VARCHAR, p1::VARCHAR, p2::VARCHAR, p3::VARCHAR FROM (VALUES (%(word_english__0)s, %(part_of_speech__0)s, %(level__0)s, %(word_portuguese__0)s, 0), (%(word_english__1)s ... 96257 characters truncated ... p0, p1, p2, p3, sen_counter) ORDER BY sen_counter RETURNING word.word_id, word.word_id AS word_id__1\n",
      "2024-03-16 15:29:09,072 INFO sqlalchemy.engine.Engine [insertmanyvalues 3/3 (ordered)] {'word_english__0': 'potential', 'part_of_speech__0': 'adjective noun', 'level__0': 'B2', 'word_portuguese__0': 'potencial', 'word_english__1': 'pound', 'part_of_speech__1': 'noun', 'level__1': 'A1', 'word_portuguese__1': 'libra', 'word_english__2': 'pour', 'part_of_speech__2': 'verb', 'level__2': 'B1', 'word_portuguese__2': 'derramar', 'word_english__3': 'poverty', 'part_of_speech__3': 'noun', 'level__3': 'B1', 'word_portuguese__3': 'pobreza', 'word_english__4': 'powder', 'part_of_speech__4': 'noun', 'level__4': 'B1', 'word_portuguese__4': 'pó', 'word_english__5': 'power', 'part_of_speech__5': 'noun, verb', 'level__5': 'A2', 'word_portuguese__5': 'poder', 'word_english__6': 'powerful', 'part_of_speech__6': 'adjective', 'level__6': 'B1', 'word_portuguese__6': 'poderoso', 'word_english__7': 'practical', 'part_of_speech__7': 'adjective', 'level__7': 'B1', 'word_portuguese__7': 'prático', 'word_english__8': 'practice', 'part_of_speech__8': 'noun', 'level__8': 'A1', 'word_portuguese__8': 'prática', 'word_english__9': 'practise', 'part_of_speech__9': 'verb', 'level__9': 'A1', 'word_portuguese__9': 'prática', 'word_english__10': 'praise', 'part_of_speech__10': 'noun verb', 'level__10': 'B2', 'word_portuguese__10': 'elogio', 'word_english__11': 'pray', 'part_of_speech__11': 'verb', 'level__11': 'B1', 'word_portuguese__11': 'orar', 'word_english__12': 'prayer', 'part_of_speech__12': 'noun' ... 3896 parameters truncated ... 'level__986': 'A1', 'word_portuguese__986': 'ano', 'word_english__987': 'yellow', 'part_of_speech__987': 'adjective, noun', 'level__987': 'A1', 'word_portuguese__987': 'amarelo', 'word_english__988': 'yes', 'part_of_speech__988': 'exclam.', 'level__988': 'A1', 'word_portuguese__988': 'sim', 'word_english__989': 'yesterday', 'part_of_speech__989': 'adverb noun', 'level__989': 'A1', 'word_portuguese__989': 'ontem', 'word_english__990': 'yet', 'part_of_speech__990': 'adverb conj.', 'level__990': 'A2', 'word_portuguese__990': 'ainda', 'word_english__991': 'you', 'part_of_speech__991': 'pronoun', 'level__991': 'A1', 'word_portuguese__991': 'vocês', 'word_english__992': 'young', 'part_of_speech__992': 'adjective, noun', 'level__992': 'A1', 'word_portuguese__992': 'jovem', 'word_english__993': 'your', 'part_of_speech__993': 'det.', 'level__993': 'A1', 'word_portuguese__993': 'seu', 'word_english__994': 'yours', 'part_of_speech__994': 'pronoun', 'level__994': 'A2', 'word_portuguese__994': 'sua', 'word_english__995': 'yourself', 'part_of_speech__995': 'pronoun', 'level__995': 'A1', 'word_portuguese__995': 'você mesmo', 'word_english__996': 'youth', 'part_of_speech__996': 'noun', 'level__996': 'B1', 'word_portuguese__996': 'juventude', 'word_english__997': 'zero', 'part_of_speech__997': 'number', 'level__997': 'A2', 'word_portuguese__997': 'zero', 'word_english__998': 'zone', 'part_of_speech__998': 'noun', 'level__998': 'B2', 'word_portuguese__998': 'zona'}\n",
      "2024-03-16 15:29:09,189 INFO sqlalchemy.engine.Engine COMMIT\n"
     ]
    }
   ],
   "source": [
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "\n",
    "# Assuming words_df is your DataFrame and already loaded\n",
    "for index, row in words_df.iterrows():\n",
    "    word = Word(\n",
    "        word_english=row['word_english'],\n",
    "        part_of_speech=row['part_of_speech'],\n",
    "        level=row['level'],\n",
    "        word_portuguese=row['word_portuguese']\n",
    "    )\n",
    "    session.add(word)\n",
    "\n",
    "session.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-16 15:42:37,853 INFO sqlalchemy.engine.Engine ROLLBACK\n",
      "2024-03-16 15:42:37,893 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2024-03-16 15:42:37,894 INFO sqlalchemy.engine.Engine \n",
      "    SELECT\n",
      "        word_portuguese\n",
      "    FROM word\n",
      "    WHERE level = 'A1'\n",
      "    ORDER BY word_id ASC\n",
      "    LIMIT 5;\n",
      "\n",
      "2024-03-16 15:42:37,895 INFO sqlalchemy.engine.Engine [cached since 52.07s ago] {}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['um', 'sobre', 'acima', 'através', 'açao']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select five words from the words.db that have the lowest level in 'level_lowest' column and have the lowest 'word_id' number\n",
    "query_message_table = text(\"\"\"\n",
    "    SELECT\n",
    "        word_portuguese\n",
    "    FROM word\n",
    "    WHERE level = 'A1'\n",
    "    ORDER BY word_id ASC\n",
    "    LIMIT 5;\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "connection.rollback()\n",
    "# connection.rollback()\n",
    "# execute the query\n",
    "result = connection.execute(query_message_table)\n",
    "# fetch the result\n",
    "\n",
    "# save the five words in a list\n",
    "five_words = [word[0] for word in result.fetchall()]\n",
    "\n",
    "five_words\n",
    "\n",
    "# connection.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# connect to openai to create a paragraph of text using the five words\n",
    "import openai\n",
    "import streamlit as st\n",
    "import gradio as gr\n",
    "import openai, subprocess\n",
    "import os\n",
    "\n",
    "openai.api_key = st.secrets.openai_api_key\n",
    "# openai.api_key = config.OPENAI_API_KEY\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": 'You are a expert portuguese tutor. Respond to all prompts with one paragraph.'}]\n",
    "\n",
    "def transcribe(audio):\n",
    "    global messages\n",
    "\n",
    "    audio_filename_with_extension = audio + '.wav'\n",
    "    os.rename(audio, audio_filename_with_extension)\n",
    "    \n",
    "    audio_file = open(audio_filename_with_extension, \"rb\")\n",
    "    transcript = openai.Audio.transcribe(\"whisper-1\", audio_file)\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": transcript[\"text\"]})\n",
    "\n",
    "    response = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=messages)\n",
    "\n",
    "    system_message = response[\"choices\"][0][\"message\"]\n",
    "    messages.append(system_message)\n",
    "\n",
    "    subprocess.call([\"say\", system_message['content']])\n",
    "\n",
    "    chat_transcript = \"\"\n",
    "    for message in messages:\n",
    "        if message['role'] != 'system':\n",
    "            chat_transcript += message['role'] + \": \" + message['content'] + \"\\n\\n\"\n",
    "\n",
    "    return chat_transcript\n",
    "\n",
    "ui = gr.Interface(fn=transcribe, inputs=gr.Audio(source=\"microphone\", type=\"filepath\"), outputs=\"text\").launch()\n",
    "ui\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe from the log.csv file with headers being in row 0\n",
    "log_df = pd.read_csv('flagged/log.csv', header=0)\n",
    "# log_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase all words in the 'output' column\n",
    "log_df['output'] = log_df['output'].str.lower()\n",
    "\n",
    "# create a dataframe from the words.db table\n",
    "words_df = pd.read_sql_query('''\n",
    "    SELECT \n",
    "        *\n",
    "    FROM words\n",
    "            ''', conn)\n",
    "\n",
    "# words_df.head()\n",
    "\n",
    "# create a function that checks all of the words in the output file to see if they are present in the words.db. if they are create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column called count to the words_df dataframe that counts the number of times the word appears in the output file\n",
    "words_df['count'] = 0\n",
    "\n",
    "# create a function that checks all of the words in the output file to see if they are present in the words_df['words'] column. if they are present, add 1 to the count column for that word\n",
    "def check_words(output):\n",
    "    for word in output.split():\n",
    "        # print(word)\n",
    "        if word in words_df['portuguese_meaning'].values:\n",
    "            words_df.loc[words_df['portuguese_meaning'] == word, 'count'] += 1\n",
    "\n",
    "log_df['output'].apply(check_words)\n",
    "\n",
    "# Create a dataframe that only contains the words that were found in the output file\n",
    "words_found_df = words_df[words_df['count'] > 0]\n",
    "\n",
    "# words_found_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creat a pie chart showing percentage of words with count > 0 out of the total number of words in the words_df dataframe\n",
    "# %matplotlib inline\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# plt.pie([len(words_found_df), len(words_df) - len(words_found_df)], labels=['Found', 'Not Found'], autopct='%1.1f%%')\n",
    "# plt.title('Words Found in Output File')\n",
    "\n",
    "# # If you're in a Jupyter Notebook or similar, the chart will display inline below the cell.\n",
    "# # If you also want to save the figure to a file, uncomment the line below:\n",
    "# # plt.savefig(\"pie_chart.png\")\n",
    "\n",
    "# plt.show()\n",
    "# plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ipywidgets as widgets\n",
    "# from IPython.display import display, clear_output\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # data\n",
    "# words_df_length = len(words_df)\n",
    "# words_found_df_length = len(words_found_df)\n",
    "# print(words_df_length)\n",
    "# print(words_found_df_length)\n",
    "# # Function to draw the pie chart\n",
    "# def draw_pie_chart(words_found, total_words):\n",
    "#     plt.figure(figsize=(10, 10))\n",
    "#     plt.pie([words_found, total_words - words_found], labels=['Found', 'Not Found'], autopct='%1.1f%%')\n",
    "#     plt.title('Words Found in Output File')\n",
    "#     plt.show()\n",
    "\n",
    "# # # Button callback\n",
    "# # def on_button_click(button):\n",
    "# #     global words_found_df_length\n",
    "# #     words_found_df_length += 100  # Simulating an update, adjust as needed\n",
    "# #     clear_output(wait=True) # Clear the previous chart\n",
    "# #     draw_pie_chart(words_found_df_length, words_df_length)\n",
    "# #     display(button)\n",
    "\n",
    "# # Button callback\n",
    "# def on_button_click(button):\n",
    "#     global words_found_df_length\n",
    "#     print(f\"Before Update: {words_found_df_length}\")  # Diagnostic print\n",
    "#     words_found_df_length += 500  # Larger increment for testing\n",
    "#     clear_output(wait=True) # Clear the previous chart and print statements\n",
    "#     draw_pie_chart(words_found_df_length, words_df_length)\n",
    "#     print(f\"After Update: {words_found_df_length}\")  # Diagnostic print\n",
    "#     display(button)\n",
    "\n",
    "# # Initial pie chart drawing\n",
    "# draw_pie_chart(words_found_df_length, words_df_length)\n",
    "\n",
    "# # Button for updating the pie chart\n",
    "# button = widgets.Button(description=\"Update Chart\")\n",
    "# button.on_click(on_button_click)\n",
    "# display(button)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ipywidgets as widgets\n",
    "# from IPython.display import display, clear_output\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # data\n",
    "# words_df_length = len(words_df)\n",
    "# words_found_df_length = len(words_found_df)\n",
    "\n",
    "# # Function to draw the pie chart\n",
    "# def draw_pie_chart(words_found, total_words):\n",
    "#     plt.figure(figsize=(10, 10))\n",
    "#     plt.pie([words_found, total_words - words_found], labels=['Found', 'Not Found'], autopct='%1.1f%%')\n",
    "#     plt.title('Words Found in Output File')\n",
    "#     plt.show()\n",
    "\n",
    "# # Create label widgets for displaying update status\n",
    "# before_update_label = widgets.Label()\n",
    "# after_update_label = widgets.Label()\n",
    "\n",
    "# # Button callback\n",
    "# def on_button_click(button):\n",
    "#     global words_found_df_length\n",
    "#     before_update_label.value = f\"Before Update: {words_found_df_length}\"  # Update label text\n",
    "#     words_found_df_length += 500  # Larger increment for testing\n",
    "#     clear_output(wait=True) # Clear the previous chart\n",
    "#     draw_pie_chart(words_found_df_length, words_df_length)\n",
    "#     after_update_label.value = f\"After Update: {words_found_df_length}\"  # Update label text\n",
    "#     display(before_update_label)\n",
    "#     display(after_update_label)\n",
    "#     display(button)\n",
    "\n",
    "# # Initial pie chart drawing\n",
    "# draw_pie_chart(words_found_df_length, words_df_length)\n",
    "\n",
    "# # Button for updating the pie chart\n",
    "# button = widgets.Button(description=\"Update Chart\")\n",
    "# button.on_click(on_button_click)\n",
    "# display(before_update_label)\n",
    "# display(after_update_label)\n",
    "# display(button)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cdde23562444851b8e8f7272049d4be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f53a68e273d844918feabebc7d71b145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51d0b8f6e916448f937f9c4a073488f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc2e95e803ec4364a786cc33c9fd86bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Update Chart', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# data\n",
    "words_df_length = len(words_df)\n",
    "words_found_df_length = len(words_found_df)\n",
    "\n",
    "# Function to draw the pie chart\n",
    "def draw_pie_chart(words_found, total_words):\n",
    "    with pie_chart_output:\n",
    "        clear_output(wait=True)  # Clear previous chart\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.pie([words_found, total_words - words_found], labels=['Found', 'Not Found'], autopct='%1.1f%%')\n",
    "        plt.title('Words Found in Output File')\n",
    "        plt.show()\n",
    "\n",
    "# Create label widgets for displaying update status\n",
    "before_update_label = widgets.Label()\n",
    "after_update_label = widgets.Label()\n",
    "\n",
    "# Button callback\n",
    "def on_button_click(button):\n",
    "    global words_found_df_length\n",
    "    before_update_label.value = f\"Before Update: {words_found_df_length}\"  # Update label text\n",
    "    words_found_df_length += 500  # Larger increment for testing\n",
    "    draw_pie_chart(words_found_df_length, words_df_length)\n",
    "    after_update_label.value = f\"After Update: {words_found_df_length}\"  # Update label text\n",
    "\n",
    "# Create an output widget to contain the pie chart\n",
    "pie_chart_output = widgets.Output()\n",
    "\n",
    "# Initial pie chart drawing\n",
    "with pie_chart_output:\n",
    "    draw_pie_chart(words_found_df_length, words_df_length)\n",
    "\n",
    "# Display everything\n",
    "display(before_update_label)\n",
    "display(after_update_label)\n",
    "display(pie_chart_output)\n",
    "button = widgets.Button(description=\"Update Chart\")\n",
    "button.on_click(on_button_click)\n",
    "display(button)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "connection to server at \"localhost\" (::1), port 5432 failed: FATAL:  database \"language_app\" does not exist\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/3l/0kvmql1518x0p2_k9tq2f_lw0000gn/T/ipykernel_54975/2690345504.py\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Connect to the database\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m conn = psycopg2.connect(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mhost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"localhost\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mport\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5432\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/clones/learn_a_language_first/.venv/lib/python3.9/site-packages/psycopg2/__init__.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(dsn, connection_factory, cursor_factory, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0mdsn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dsn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m     \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconnection_factory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconnection_factory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwasync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcursor_factory\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcursor_factory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcursor_factory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOperationalError\u001b[0m: connection to server at \"localhost\" (::1), port 5432 failed: FATAL:  database \"language_app\" does not exist\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "# Connect to the database\n",
    "conn = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    port=5432,\n",
    "    database=\"language_app\",\n",
    "    user=\"christopherdedow\",\n",
    "    password=\"codingmode23\")\n",
    "\n",
    "# Create a cursor. this does not return data, it returns a cursor object which can be used to perform queries\n",
    "cur = conn.cursor()\n",
    "\n",
    "# check what data is already in the table and save it to a dataframe\n",
    "log_text_df = pd.read_sql_query('''\n",
    "    SELECT \n",
    "        *\n",
    "    FROM log\n",
    "            ''', conn)\n",
    "\n",
    "log_text_df.head()\n",
    "\n",
    "# # Write a row to the table\n",
    "# cur.execute(\"INSERT INTO my_table (name, age) VALUES ('John Doe', 30)\")\n",
    "\n",
    "# # Commit the changes\n",
    "# conn.commit()\n",
    "\n",
    "# # Close the connection\n",
    "# conn.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
